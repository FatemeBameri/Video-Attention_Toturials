# Video-Attention
Summary of related papers on vido attention. 
## Video attention Papers

* Recurrent attention unit: A new gated recurrent unit for long-term memory of important parts in sequential data (Neurocomputing 2023) [pdf](https://www.sciencedirect.com/science/article/abs/pii/S0925231222013339)
* Attention mechanisms in computer vision: A survey (Computational Visual Media 2022) [pdf](https://link.springer.com/article/10.1007/s41095-022-0271-y) 
*  Attention for vision-based assistive and automated driving: A review of algorithms and datasets (IEEE Transactions on Intelligent Transportation Systems 2022) [pdf](https://ieeexplore.ieee.org/abstract/document/9827989/)
*  Transformers in vision: A survey (ACM computing surveys 2022) [pdf](https://dl.acm.org/doi/abs/10.1145/3505244)
*  Attention, please! A survey of neural attention models in deep learning (Artificial Intelligence Review 2022) [pdf](https://link.springer.com/article/10.1007/s10462-022-10148-x)
* A review on the attention mechanism of deep learning (Neurocomputing 2021) [pdf](https://www.sciencedirect.com/science/article/abs/pii/S092523122100477X)
*  An attentive survey of attention models (ACM Transactions on Intelligent Systems and Technology 2021) [pdf](https://dl.acm.org/doi/abs/10.1145/3465055)
HSTA: A hierarchical spatio-temporal attention model for trajectory prediction (IEEE Transactions on Vehicular Technology 2021) [pdf](https://ieeexplore.ieee.org/abstract/document/9548801)
* Attention in natural language processing (IEEE transactions on neural networks and learning systems 2020) [pdf](https://ieeexplore.ieee.org/abstract/document/9194070)
*  Attention in psychology, neuroscience, and machine learning (Frontiers in computational neuroscience 2020) [pdf](https://www.frontiersin.org/articles/10.3389/fncom.2020.00029/full)
* Recurrent spatial-temporal attention network for action recognition in videos (IEEE Transactions on Image Processing 2017) [pdf](https://ieeexplore.ieee.org/abstract/document/8123939)
* Survey of spatio-temporal interest point detection algorithms in video (IEEE Access 2017) [pdf](https://ieeexplore.ieee.org/abstract/document/7944559)
* Typologies of attentional networks (Nature reviews neuroscience 2006) [pdf](https://www.nature.com/articles/nrn1903)
* Neural Machine Translation by Jointly Learning to Align and Translate
*  Recurrent Models of Visual Attention
*  Multiple Object Recognition with Visual Attention
*  Effective Approaches to Attention-based Neural Machine Translation

## Understanding Attention
* General meaning of attention in wikipedia [link](https://en.wikipedia.org/wiki/Attention_(machine_learning))
* Attention and Memory in Deep Learning [Youtube](https://www.youtube.com/watch?v=AIiwuClvH6k)
* Comprehensive Guide to Attention Mechanism [blog](https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/)
* Attention Mechanism from Scratch [link](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/)
* Transformer Attention Mechanism [link](https://machinelearningmastery.com/the-transformer-attention-mechanism/)
* Positional Encoding in Transformer Models [link](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
* Training the Transformer Model [link](https://machinelearningmastery.com/training-the-transformer-model/)
* Transformer Model [link](https://machinelearningmastery.com/the-transformer-model/)
* Scaled Dot-Product Attention [link](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras/)
* Attention in Long Short-Term Memory Recurrent Neural Networks [link](https://machinelearningmastery.com/attention-long-short-term-memory-recurrent-neural-networks/)

## Pytorch Code for Attention
* Transformers [(github)](https://github.com/huggingface/transformers)
* Attention Gated Networks [(github)](https://github.com/ozan-oktay/Attention-Gated-Networks)
* ResNeSt: Split-Attention Networks [(github)](https://github.com/zhanghang1989/ResNeSt)
* Self-Attention GAN [(github)](https://github.com/heykeetae/Self-Attention-GAN)
* Attention Transfer [(github)](https://github.com/szagoruyko/attention-transfer)
* Dual Attention Network [(github)](https://github.com/junfu1115/DANet)
* ECA-Net: Efficient Channel Attention [(github)](https://github.com/BangguWu/ECANet)
* RCAN [(github)](https://github.com/yulunzhang/RCAN)
* U-GAT-IT [(github)](https://github.com/znxlwm/UGATIT-pytorch)
* PSEnet+CRNN [(github)](https://github.com/rahzaazhar/PAN-PSEnet)
* seq2seqModel [(github)](https://github.com/sudhirNallam/seq2seqModel)
* xformers_mingpt [(colab)](https://colab.research.google.com/github/facebookresearch/xformers/blob/main/docs/source/xformers_mingpt.ipynb)
* 

## Papers with Code
* A Structured Self-attentive Sentence Embedding [(code)](https://github.com/kaushalshetty/Structured-Self-Attention)
* SA-Net: Shuffle Attention for Deep Convolutional Neural Networks [(code)](https://github.com/wofmanaf/SA-Net)
* Contextual transformer networks for visual recognition [(code)](https://github.com/yehli/imagenetmodel)
* Fully Attentional Networks [(code)](https://github.com/nvlabs/fan)


